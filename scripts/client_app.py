"""bouquetfl: A Flower / PyTorch app."""

import time
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import logging
from flwr.app import Message

from flwr.client import Client, ClientApp

from flwr.common import (Code, Context, EvaluateIns, EvaluateRes, FitIns,
                         FitRes, Status, parameters_to_ndarrays)

from bouquetfl.core.create_env import run_training_process_in_env
from bouquetfl.utils import power_clock_tools as pct
from bouquetfl.utils.filesystem import save_ndarrays

from flwr.app import ArrayRecord, Context, Message, MetricRecord, RecordDict
from flwr.clientapp import ClientApp
logger = logging.getLogger(__name__)

import torch
from task import cifar10 as flower_baseline

#####################################
########## Flower Client ############
#####################################

# Flower ClientApp
app = ClientApp()


# Define Flower Client and client_fn

@app.train()

def train(msg: Message, context: Context):
    """Train the model on local data."""

    state_dict = msg.content["arrays"].to_torch_state_dict()
    torch.save(state_dict, f"/tmp/global_params_round_{msg.content['config']['server-round']}.pt")

    status, state_dict_updated = run_training_process_in_env(msg=msg, context=context)
    if status.code != Code.OK:
        raise torch.OutOfMemoryError(f"{"\033[31m"}Client {context.node_config['partition-id']} has encountered an out-of-memory error{"\033[0m"}")
    '''else:
        testset = flower_baseline.load_global_test_data()

        model = flower_baseline.get_model()
        model.load_state_dict(state_dict_updated)
        loss, accuracy = flower_baseline.test(
            model=model,
            testloader=testset,
            device="cuda" if torch.cuda.is_available() else "cpu",
        )
        print(f"{"\033[32m"}Client {context.node_config['partition-id']} evaluation{"\033[0m"}: accuracy: {round(100 * accuracy, 2)}%, loss: {round(loss, 2)}")
    '''
    # Construct and return reply Message
    model_record = ArrayRecord(torch_state_dict = state_dict_updated)
    metrics = {
        "num-examples": 1,
    }
    metric_record = MetricRecord(metrics)
    content = RecordDict({"arrays": model_record, "metrics": metric_record})
    return Message(content=content, reply_to=msg)

@app.evaluate()
def evaluate(msg: Message, context: Context):
    """Evaluate the model on local test data."""

    state_dict = msg.content["arrays"].to_torch_state_dict()

    testset = flower_baseline.load_data(partition_id=context.node_config["partition-id"], num_clients = context.run_config["num-clients"], num_workers=4, batch_size=context.run_config["batch-size"])

    model = flower_baseline.get_model()
    model.load_state_dict(state_dict)
    loss, accuracy = flower_baseline.test(
        model=model,
        testloader=testset,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    print(f"{"\033[32m"}Global model evaluation on test set {context.node_config['partition-id']}{"\033[0m"}: accuracy: {round(100 * accuracy, 2)}%, loss: {round(loss, 2)}")
    time.sleep(0.5)

    # Construct and return reply Message
    metrics = {
        "loss": loss,
        "accuracy": accuracy,
        "num-examples": 1,
    }
    metric_record = MetricRecord(metrics)
    content = RecordDict({"metrics": metric_record})
    return Message(content=content, reply_to=msg)